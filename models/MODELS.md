# 2024-2025 Model Options (Ranked by Quality)

The core requirements are: Gramatically correct, culturally and contextually aware translation EN to RO, uncensored, able to translate explicit adult content, able to use correct declensions, conjugations, syntax and topic in Romanian. They must run on a Windows PC with 16GB RAM, RTX3060 with 6GB VRAM + shared VRAM.

## Overview Table

| Model | Type | Params (B, billions) | BLEU Score | Tatoeba Score | Flores Score | VRAM GB required | Notes |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **[LLMic 3B](https://huggingface.co/faur-ai/LLMic)** | Bilingual Ro-En <br> safetensors | 3 | __41.01__ (WMT16) | | | 3.5 |ğŸ‘ Best EN-RO BLEU score, massive RO corpus.<br>ğŸ‘ Needs GGUF quantization.  Best RO-specific if quantized. |
| **[Aya-23-8B](https://huggingface.co/cohere/aya-23-8B)** | Multilingual LLM <br> GGUF | 8 | | | 34.8 | 5.8 |ğŸ‘ Uncensored, GGUF, 23 languages <br>ğŸ‘ Slower, larger VRAM |
| **[MBART-En-Ro](https://huggingface.co/facebook/mbart-large-en-ro)** | __Ro-Translation__ <br> safetensors | 0.6 | __38.0__ (WMT16) | | | 2 |ğŸ‘ Largest RO-specific, good context.<br>ğŸ‘ Smaller than multilinguals |
| **[MADLAD-400](https://huggingface.co/google/madlad-400-3b-mt)** | Translations <br> safetensors | 3 | ~35.11 | | 38.4 | 4 |ğŸ‘ Uncensored, safetensors, 400+ languages <br>ğŸ‘ Requires `trust_remote_code`, lower quality for some languages |
| **[SeamlessM4T-v2](https://huggingface.co/facebook/seamless-m4t-v2-large)** | Multimodal <br> safetensors | 2.3 | | | 38.8 | 5+ |ğŸ‘ Most recent from Meta, better than NLLB.<br>ğŸ‘ Includes unneeded speech features |
| **[QuickMT-En-Ro](https://huggingface.co/quickmt/quickmt-en-ro)**| __Ro-Translation__ <br> safetensors | ? | | | 42.29 | 0.5 |ğŸ‘ Most recent RO model (Oct 2024).<br>ğŸ‘ Unproven, unknown architecture.  Experimental. |
| **[NLLB-200](https://huggingface.co/facebook/nllb-200-3.3B)** | Translations <br> safetensors | 3.3 | ~31.17 | | 37.5 | 4.5 |ğŸ‘ Proven, stable, large community.<br>ğŸ‘ Older model (2022).  Good for reliability. |
| **[OPUS-MT-TC-Big](https://huggingface.co/Helsinki-NLP/opus-mt-tc-big-en-ro)**| Large OPUS <br> safetensors | 0.2 | 34.0 (Newstest2016) | 48.6 | 40.4 | 1 |ğŸ‘ Good grammar for size, small footprint.<br>ğŸ‘ Smaller than MBART, may be censored.  Good for low VRAM. |
| **[Helsinki-Tatoeba](https://huggingface.co/Helsinki-NLP/opus-tatoeba-en-ro)**| Transformer-align <br> safetensors | 0.078 | 31.7 (Newstest2016) | 46.9 | | 0.2 |ğŸ‘ Better than standard OPUS, tiny footprint.<br>ğŸ‘ Small model, not for complex grammar.  Requires `>>ron<<` token. |
| **[suzume-llama-3-8B](https://huggingface.co/lightblue/suzume-llama-3-8B-multilingual)**| Multilingual LLM <br> safetensors | 8 | | | | ~5-6 |ğŸ‘ Based on powerful Llama 3, likely uncensored, very new (Oct 2024).<br>ğŸ‘ Romanian is not a focus, EN-RO performance is unknown.  Experimental high-potential |
| **[Marcoroni-7B-v3](https://huggingface.co/TheBloke/Marcoroni-7B-v3-GGUF)**| Instruct LLM <br> GGUF | 7 | | | | ~4.8 |ğŸ‘ Strong Mistral base, likely uncensored, was #1 on 7B leaderboard.<br>ğŸ‘ Not for translations, for general tasks. EN-RO performance is unknown.  Experimental. |
| **[OLMo-7B](https://huggingface.co/allenai/OLMo-7B)** | Multilingual LLM <br> safetensors | 7 | | | | 5 |ğŸ‘ Fully open source.<br>ğŸ‘ Research-focused, may not match SOTA. For open-source enthusiasts. |
| **[BlackKakapo-MT](https://huggingface.co/BlackKakapo/opus-mt-en-ro)**| Community OPUS <br> safetensors | 0.075 | ~24.5 (Estimated) | 53.1 | | 0.5 |ğŸ‘ Community fine-tuned.<br>ğŸ‘ Single-person project, weakest grammar.  For extreme VRAM constraints. |
| **[Orion-14B](https://huggingface.co/OrionStarAI/Orion-14B)** | Multilingual LLM <br> safetensors | 14 | | | | 9 |ğŸ‘ Large context window.<br>ğŸ‘ Too heavy for 6GB VRAM. |
| **[OpenELM-3B](https://huggingface.co/apple/OpenELM-3B)** | Multilingual LLM <br> safetensors | 3 | | | | 2.5 |ğŸ‘ Very fast and lightweight.<br>ğŸ‘ Too small for complex Romanian.  NOT RECOMMENDED. |

---

## Model Types Explained
- **Multilingual LLM:** General-purpose Large Language Models trained on many languages (e.g., Aya, Orion). They are good at understanding context but are not exclusively built for translation.
- **Instruct LLM:** A general-purpose LLM that has been fine-tuned to be good at following user commands or "instructions." Their translation ability varies.
- **Translations / Ro-Translations:** Models designed and trained specifically for translation tasks, either between many languages (Translations) or focused on Romanian (Ro-Translations).
- **Bilingual Ro-En:** Foundation models trained extensively on both Romanian and English, making them highly effective for translation between the two.
- **Multimodal:** Models that can process more than one type of data, such as both text and audio (e.g., SeamlessM4T).
- **OPUS / Transformer-align:** Architectures that are highly effective for translation. OPUS is a popular framework, and many models are built on it, sometimes with community fine-tuning.

